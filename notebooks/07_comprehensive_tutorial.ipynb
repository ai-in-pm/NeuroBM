{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete NeuroBM Tutorial: From Theory to Practice\n",
    "\n",
    "**Educational/Research Platform for Cognitive Modeling**\n",
    "\n",
    "This comprehensive tutorial covers the entire NeuroBM workflow from theoretical foundations to practical implementation and interpretation.\n",
    "\n",
    "## üéØ Tutorial Objectives\n",
    "\n",
    "1. **Understand** Boltzmann machine theory and cognitive modeling applications\n",
    "2. **Implement** complete training pipelines for different scenarios\n",
    "3. **Analyze** learned representations and patterns\n",
    "4. **Interpret** results with proper limitations and ethical considerations\n",
    "5. **Apply** best practices for research and education\n",
    "\n",
    "## ‚ö†Ô∏è Important Disclaimers\n",
    "\n",
    "- **Educational Purpose Only**: All examples are for learning and research\n",
    "- **No Clinical Applications**: Not for diagnosis, treatment, or medical decisions\n",
    "- **Synthetic Data**: All data is artificially generated\n",
    "- **Theoretical Models**: Simplified representations of complex phenomena\n",
    "- **Validation Required**: Real applications need proper validation studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete imports for the tutorial\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# NeuroBM complete imports\n",
    "from neurobm.models.rbm import RestrictedBoltzmannMachine\n",
    "from neurobm.models.dbm import DeepBoltzmannMachine\n",
    "from neurobm.models.crbm import ConditionalRBM\n",
    "from neurobm.data.synth import SyntheticDataGenerator\n",
    "from neurobm.data.loaders import get_data_loader, get_sequence_loader\n",
    "from neurobm.data.schema import get_schema\n",
    "from neurobm.data.transforms import create_preprocessing_pipeline\n",
    "from neurobm.training.loop import TrainingLoop\n",
    "from neurobm.training.callbacks import get_standard_callbacks\n",
    "from neurobm.training.eval import ModelEvaluator\n",
    "from neurobm.interpret.saliency import SaliencyAnalyzer\n",
    "from neurobm.interpret.mutual_info import MutualInformationAnalyzer\n",
    "from neurobm.interpret.traversals import LatentTraverser\n",
    "from neurobm.interpret.tiles import FilterVisualizer\n",
    "\n",
    "# Setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ NeuroBM Tutorial Environment Ready\")\n",
    "print(f\"üì± Device: {device}\")\n",
    "print(f\"üêç Python: {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print(f\"\\nüß† Ready to explore cognitive modeling with Boltzmann machines!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Data\n",
    "\n",
    "Let's start by exploring the different cognitive regimes available in NeuroBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore available regimes\n",
    "regimes = ['base', 'ptsd', 'autism', 'ai_reliance']\n",
    "\n",
    "print(\"üóÇÔ∏è Available Cognitive Regimes:\\n\")\n",
    "\n",
    "regime_info = {}\n",
    "for regime in regimes:\n",
    "    try:\n",
    "        schema = get_schema(regime)\n",
    "        features = list(schema.features.keys())\n",
    "        regime_info[regime] = {\n",
    "            'features': features,\n",
    "            'n_features': len(features),\n",
    "            'description': schema.description if hasattr(schema, 'description') else f'{regime.title()} cognitive features'\n",
    "        }\n",
    "        \n",
    "        print(f\"üìã {regime.upper()}:\")\n",
    "        print(f\"   Features ({len(features)}): {', '.join(features)}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not load {regime}: {e}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for regime, info in regime_info.items():\n",
    "    comparison_data.append({\n",
    "        'Regime': regime.title(),\n",
    "        'Features': info['n_features'],\n",
    "        'Focus': info['description']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"üìä Regime Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Comparison Workflow\n",
    "\n",
    "Let's implement a complete workflow that compares different models on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for comparison\n",
    "print(\"üî¨ Generating Comparison Dataset...\\n\")\n",
    "\n",
    "# Use base regime for comparison\n",
    "generator = SyntheticDataGenerator('base', random_seed=42)\n",
    "data = generator.generate_samples(n_samples=2000, method='skewed')\n",
    "feature_names = list(generator.schema.features.keys())\n",
    "\n",
    "# Split data\n",
    "train_data = data[:1600]  # 80% for training\n",
    "test_data = data[1600:]   # 20% for testing\n",
    "\n",
    "print(f\"üìä Dataset Summary:\")\n",
    "print(f\"‚Ä¢ Total samples: {len(data)}\")\n",
    "print(f\"‚Ä¢ Training samples: {len(train_data)}\")\n",
    "print(f\"‚Ä¢ Test samples: {len(test_data)}\")\n",
    "print(f\"‚Ä¢ Features: {len(feature_names)}\")\n",
    "print(f\"‚Ä¢ Feature names: {feature_names}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(train_data),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(test_data),\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nüîÑ Data Loaders Created:\")\n",
    "print(f\"‚Ä¢ Train batches: {len(train_loader)}\")\n",
    "print(f\"‚Ä¢ Test batches: {len(test_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
